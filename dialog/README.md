# Проект: Создание/разработка скрипта для анализа переписок в чатах.
Цели проекта:
* Повысить вовлеченность в жизнь сообщества
* Снизить операционную нагрузку на комьюнити менеджеров на 30%
а также поднять конверсию вступления в выпускные комьюнити на +5%

Какие основные функции должны быть реализованы? 
* инструмент для аналитики переписки
* суммаризация полученной информации
* написание итогов обсуждений в 2х форматах:
  1. дайджест для участников (с настройкой периодичности: обсуждения за день/за неделю/за месяц) со ссылками на сообщения участников 
  2. сбор информации для комьюнити-менеджеров: рейтинг тем, которые обсуждали в сообществе с частотой обсуждений

Какую структуру и какие разделы необходимо реализовать в проекте?  
Результатом проекта является скрипт, который выдает результаты, с разбивкой на следующие этапы :
* инструмент для аналитики переписки 
* суммаризация полученной информации 
* написание итогов обсуждений в 2х формата: 
дайджест для участников (с настройкой периодичности: обсуждения за день/за неделю/за месяц) со ссылками на сообщения участников и сбор информации для комьюнити-менеджеров: рейтинг тем, которые обсуждали в сообществе с частотой обсуждений


Аналитика сообщений выполняется скриптом, результаты которого сохраняются в виде CSV-файла с четко определёнными столбцами. Скрипт должен собирать данные о количестве сообщений в чате за заданные временные промежутки с возможностью гибкой настройки периодичности запуска: обычно один раз в месяц, иногда раз в неделю и, в редких случаях, ежедневно. 

# Парсинг
Вывод: В резельтате парсинга json-файла про маркетинг. Получил датафрейм с признаками
* message_id - индекс сообщения
* date - время
* chat_name - название чата
* chat_id	- индекс чата
* sender_id	- индекс пользователя
* from - ФИ пользователя
* text - текст сообщения
* 3680 rows × 7 columns

# Метод №1. Кластеризация по reply и по соседним сообщениям с временным threshold`ом
Суть подхода:  
Необходимо выделить в кластеры сообщения, связанные между собой с помощью механики reply. Также добавить в эти кластеры соседние сообщения с маленьким временным порогом. Я использовал th_minute=0.5.  


План реализации:
* добавить в датафрейм информацию об reply
* собрать в список пары индексов сообщений связанных reply
* добавить в список пары индексов соседних сообщений (30 секунд)
* разбиваем сообщения на кластеры с помощью полученных связей через networkx и Graf
* топ 10 диалогов по количеству сообщений
* анализ результата

Вывод:
* Провел кластеризацию по reply и по соседним сообщениям с временным threshold`ом.
* Порог по времени пол минуты. Это порог выбрал вручную сравнивая с количеством полученных пар. При этом пороге 1500 пар из датасета в 3680
* Собрал диалоги(кластеры) из 3809 связей(пар)
* 3459 сообщение (из 3680) попали в диалоги
* Получил 341 диалог.
* Выделил самые длинные (10шт).
* 103 собщения в самом большом, 51 в маленьком.
* Метрику оценил пока только визульно(почитал сообщения в диалоге). Связь между сообщениями прослеживается.

# Метод № 2. Кластеризация по всплескам активности по времени
Суть подхода:
Необходимо выделить в кластеры всплески активности - соседние сообщения с временным порогом. Я использовал порог в 30 мин. Если в чате пауза 30 мин, до переходим к следующему диалогу.

План реализации:

* Для определения порога провезти анализ распределения сообщений по времени
* Выбрать порог
* Добавить в список пары индексов соседних сообщений с порогом в 30 мин
* Разбиваем сообщения на кластеры с помощью полученных связей через networkx и Graf
* Топ 10 диалогов по количеству сообщений
* Анализ результата

Вывод:
* Провел кластеризацию по всплескам активности. Порог по временногу интервалу между соседними сообщениями 30 минут.
* Порог по времени 30 минут. Этот порог выбрал после анализа графиков активности
* Собрал диалоги(кластеры) из 3466 связей(пар) 
* 3619 сообщений (из 3680) попали в диалоги
* Получил 153 диалога.
* Выделил самые длинные (10шт).
* 206 собщения в самом большом, 79 в маленьком.
* Метрику оценил пока только визульно(почитал сообщения в диалоге). Связь между сообщениями прослеживается.

# Оценка и сравнение двух методов
Оценил по одному кластеру с помощью DeepSeek.

Вывод: Оценил по одному кластеру с помощью DeepSeek. Также оценил кластеры вручную. Оба метода работают хорошо. Имеем четко сформулированную тему, практически все сообщения из кластера так или иначе в теме участвуют. Мне нравится результат Метода № 1. Кластеры поменьше и подход мне кажется получьше.  

Приемущество предствленного подхода для кластеризации:
* не используем машинное обучение
* точность (не доказано)
* простота реализации и интерпретируемость

Для нашей задачи считаю приемлем использование Метода №1.

# Доработка и оценка качества кластеризации Метода №1

1. На основе метода №1 провел кластеризацию
2. Собрал Датафрейм
    * message_id
    * text
    * n_claster - номер кластера от  0 по количеству сообщений в кластере (нулевой самый большой)
    * cluster_size - количество сообщений в кластере
    * connection_count - количество связей сообщение с другими сообщениями
    * cluster_rating - рейтинг диалогов
    * topic - название темы диалога
    * words - ключевые слова
    * idea - суть диалога
Название темы диалога заполнил на основе работы llama3:8b (код выполнялся примерно 10 минут)
Запрос:  'Проанализируй следующие сообщения из одного кластера и определи основную тему диалога.
    Сообщения должны иметь общую тематику, так как они попали в один кластер. Напиши только основную тему. Без пояснений.  

    Сообщения:
    {messages}

    Основная тема диалога:'  
В качестве сообщений подовались соошения: серия сообщений message преобразованная в строку "как есть".  
Предворительно сообщения были отфильтрованы по признаку количество связей больше двух.  
Температура модели: Пониженная температура (0.3) для более предсказуемых ответов  

В результате создал два файла в Excel. Основной датафрейм и список названий кластеров.
# В готовом решении использовано:
* LLM llama3:8b работает с определением тем диалогов, ключевыые слова диалогов, суть диалогов.
* По умолчанию работаем только с первыми 20 диалогами, определенными рейтингом(колчество сообщений + колчество реакций).
* На системе с ГПУ(видеопамять 3ГБ) скрипт примерно работает 10-15 минут.
* Оптимально для такой системы запросы подавать батчами по 5шт. 
